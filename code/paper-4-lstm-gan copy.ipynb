{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow import keras\n",
    "\n",
    "import sklearn.preprocessing\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(directory):    \n",
    "    merged_df = pd.DataFrame()\n",
    "    for files in os.listdir(directory):\n",
    "        df = pd.read_csv(os.path.join(directory, files), index_col='timestamp')\n",
    "        df.dropna(inplace=True)\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_window(data, timesteps):\n",
    "    samples = data.shape[0] - timesteps + 1  # Number of samples in the new 3D array\n",
    "    variables = data.shape[1]  # Number of variables (features)\n",
    "\n",
    "    data_3d = np.zeros((samples, timesteps, variables))\n",
    "\n",
    "    for i in range(samples):\n",
    "        data_3d[i] = data[i:i+timesteps]\n",
    "    return data_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_convert_time_window(data_3d):\n",
    "    samples, timesteps, variables = data_3d.shape\n",
    "    data_2d = np.zeros((samples + timesteps - 1, variables))\n",
    "\n",
    "    count = np.zeros((samples + timesteps - 1, variables))\n",
    "\n",
    "    for i in range(samples):\n",
    "        data_2d[i:i+timesteps] += data_3d[i]\n",
    "        count[i:i+timesteps] += 1\n",
    "\n",
    "    data_2d /= count\n",
    "    return data_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reconstruction_loss(data, reconstructions):\n",
    "    reconstruction_errors = np.mean(np.abs(data - reconstructions), axis=1)\n",
    "    return reconstruction_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(reconstruction_errors, y_true):\n",
    "    best_threshold = 0.0\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    thresholds = np.linspace(0.0, 1.0, num=1000)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (reconstruction_errors > threshold).astype(bool)\n",
    "\n",
    "        current_f1 = f1_score(y_true.astype(bool), y_pred)\n",
    "\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = get_dataset('dataset_processed/splitted/2/train')\n",
    "merged_data_class = merged_df['class']\n",
    "\n",
    "merged_data_np = merged_df.drop('class', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 10\n",
    "features = merged_data_np.shape[1]   \n",
    "latent_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(merged_data_np)\n",
    "\n",
    "merged_data_np_scaled = scaler.transform(merged_data_np)\n",
    "\n",
    "pickle.dump(scaler, open('StandardScaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_np_3d = convert_time_window(merged_data_np_scaled, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(536753, 10, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data_np_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "time_steps = 100   # Number of timesteps in input data\n",
    "features = 10      # Number of features in each timestep\n",
    "latent_dim = 64    # Dimension of latent space\n",
    "\n",
    "# Encoder function\n",
    "def build_encoder(input_shape, latent_dim):\n",
    "    inputs = keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM layers for encoding\n",
    "    x = keras.layers.LSTM(128, return_sequences=True)(inputs)\n",
    "    x = keras.layers.LSTM(64)(x)\n",
    "    x = keras.layers.Dense(64)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    # Latent representation\n",
    "    latent = keras.layers.Dense(latent_dim, activation='relu')(x)\n",
    "    \n",
    "    return keras.models.Model(inputs, latent, name=\"Encoder\")\n",
    "\n",
    "# Decoder function\n",
    "def build_decoder(latent_dim, output_shape):\n",
    "    inputs = keras.layers.Input(shape=(latent_dim,))\n",
    "    \n",
    "    # Fully connected layers and Reshape to reconstruct time-series data\n",
    "    x = keras.layers.Dense(64)(inputs)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Dense(output_shape[0] * output_shape[1])(x)\n",
    "    outputs = keras.layers.Reshape(output_shape)(x)\n",
    "    \n",
    "    return keras.models.Model(inputs, outputs, name=\"Decoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator (Encoder + Latent Space + Decoder)\n",
    "def build_generator(input_shape, latent_dim):\n",
    "    encoder = build_encoder(input_shape, latent_dim)\n",
    "    decoder = build_decoder(latent_dim, input_shape)\n",
    "    \n",
    "    inputs = keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encode, apply latent space, and decode\n",
    "    latent = encoder(inputs)\n",
    "    outputs = decoder(latent)\n",
    "    \n",
    "    return keras.models.Model(inputs, outputs, name=\"Generator\")\n",
    "\n",
    "# Discriminator (Encoder + Dense Layer)\n",
    "def build_discriminator(input_shape, latent_dim):\n",
    "    encoder = build_encoder(input_shape, latent_dim)\n",
    "    \n",
    "    inputs = keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Use encoder and add classification layer\n",
    "    latent = encoder(inputs)\n",
    "    outputs = keras.layers.Dense(1, activation='sigmoid')(latent)\n",
    "    \n",
    "    return keras.models.Model(inputs, outputs, name=\"Discriminator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "def generator_loss(fake_output, reconstructed_input, real_latent, reconstructed_latent, lambdas):\n",
    "    bce_loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "    # Reconstruction loss (input space, L1 loss)\n",
    "    lx = tf.reduce_mean(tf.abs(reconstructed_input - real_latent))\n",
    "    \n",
    "    # Latent space reconstruction loss (L2 loss)\n",
    "    lz = tf.reduce_mean(tf.square(reconstructed_latent - real_latent))  # Compare the two latent vectors\n",
    "    \n",
    "    # Adversarial loss (Binary Crossentropy)\n",
    "    adversarial_loss = bce_loss(tf.ones_like(fake_output), fake_output)\n",
    "    \n",
    "    # Combine losses\n",
    "    loss = lambdas[0] * lx + lambdas[1] * lz + lambdas[2] * adversarial_loss\n",
    "    return loss\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    bce_loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "    # Binary crossentropy loss for real and fake samples\n",
    "    real_loss = bce_loss(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = bce_loss(tf.zeros_like(fake_output), fake_output)\n",
    "    \n",
    "    # Total discriminator loss\n",
    "    return real_loss + fake_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the models\n",
    "input_shape = (time_steps, features)\n",
    "latent_dim = 64\n",
    "\n",
    "generator = build_generator(input_shape, latent_dim)\n",
    "encoder2 = build_encoder(input_shape, latent_dim)  # Added Encoder2 for latent space reconstruction\n",
    "discriminator = build_discriminator(input_shape, latent_dim)\n",
    "\n",
    "# Optimizers\n",
    "gen_optimizer = keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
    "disc_optimizer = keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
    "\n",
    "# Placeholders for real and fake inputs\n",
    "real_data = keras.layers.Input(shape=input_shape)\n",
    "fake_data = generator(real_data)\n",
    "\n",
    "# Discriminator training\n",
    "real_output = discriminator(real_data)\n",
    "fake_output = discriminator(fake_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom training loop\n",
    "@tf.function\n",
    "def train_step(real_data):\n",
    "    # Latent vectors for reconstruction\n",
    "    real_latent = generator(real_data)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generate fake data\n",
    "        fake_data = generator(real_data)\n",
    "        \n",
    "        # Discriminator outputs\n",
    "        real_output = discriminator(real_data)\n",
    "        fake_output = discriminator(fake_data)\n",
    "        \n",
    "        # Latent space reconstruction using Encoder2\n",
    "        reconstructed_latent = encoder2(fake_data)\n",
    "        \n",
    "        # Calculate losses\n",
    "        gen_loss = generator_loss(fake_output, fake_data, real_latent, reconstructed_latent, lambdas=[0.1, 0.1, 0.8])\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    # Update gradients\n",
    "    gradients_of_generator = tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for real_data in dataset:\n",
    "            gen_loss, disc_loss = train_step(real_data)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Gen Loss: {gen_loss}, Disc Loss: {disc_loss}')\n",
    "\n",
    "# Anomaly score and labeling\n",
    "def compute_anomaly_score(real_data):\n",
    "    fake_data = generator.predict(real_data)\n",
    "    \n",
    "    # Latent space reconstruction loss using Encoder2\n",
    "    latent_real = generator(real_data)\n",
    "    latent_fake = encoder2(fake_data)\n",
    "    \n",
    "    anomaly_score = np.mean(np.square(latent_real - latent_fake), axis=1)\n",
    "    \n",
    "    return anomaly_score\n",
    "\n",
    "def label_data(anomaly_score, thresholds):\n",
    "    labels = np.zeros_like(anomaly_score)\n",
    "    labels[anomaly_score < thresholds[0]] = 0  # \"Good\"\n",
    "    labels[(anomaly_score >= thresholds[0]) & (anomaly_score < thresholds[1])] = 1  # \"Watching\"\n",
    "    labels[(anomaly_score >= thresholds[1]) & (anomaly_score < thresholds[2])] = 2  # \"Warning\"\n",
    "    labels[anomaly_score >= thresholds[2]] = 3  # \"Fault\"\n",
    "    return labels\n",
    "\n",
    "# Model Evaluation\n",
    "def evaluate_model(real_data, true_labels, thresholds):\n",
    "    # Compute anomaly scores\n",
    "    anomaly_scores = compute_anomaly_score(real_data)\n",
    "    \n",
    "    # Label data based on thresholds\n",
    "    predicted_labels = label_data(anomaly_scores, thresholds)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample dataset and training example\n",
    "train_data = np.random.randn(1000, time_steps, features)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(32)\n",
    "\n",
    "# Train the model\n",
    "train(train_dataset, epochs=50)\n",
    "\n",
    "# Evaluate the model (replace `true_labels` with actual labels)\n",
    "true_labels = np.random.randint(0, 4, size=1000)\n",
    "evaluate_model(train_data, true_labels, thresholds=[0.2, 0.5, 0.7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generator (Autoencoder with Encoder1 and Decoder)\n",
    "# def build_generator():\n",
    "#     inputs = keras.layers.Input(shape=(time_steps, features))\n",
    "    \n",
    "#     # Encoder 1 (LSTM layers + Fully Connected Layers)\n",
    "#     x = keras.layers.LSTM(128, return_sequences=True)(inputs)\n",
    "#     x = keras.layers.LSTM(64)(x)\n",
    "#     x = keras.layers.Dense(64)(x)\n",
    "#     x = keras.layers.BatchNormalization()(x)\n",
    "#     x = keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "#     # Latent representation (z)\n",
    "#     latent = keras.layers.Dense(latent_dim, activation='relu')(x)\n",
    "\n",
    "#     # Decoder (Symmetrical to Encoder 1)\n",
    "#     x = keras.layers.Dense(64)(latent)\n",
    "#     x = keras.layers.BatchNormalization()(x)\n",
    "#     x = keras.layers.LeakyReLU()(x)\n",
    "#     x = keras.layers.Dense(time_steps * features)(x)\n",
    "#     outputs = keras.layers.Reshape((time_steps, features))(x)\n",
    "    \n",
    "#     model = keras.models.Model(inputs, outputs, name=\"Generator\")\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # Discriminator (used to classify real vs fake)\n",
    "# def build_discriminator():\n",
    "#     inputs = keras.layers.Input(shape=(time_steps, features))\n",
    "    \n",
    "#     x = keras.layers.LSTM(128, return_sequences=True)(inputs)\n",
    "#     x = keras.layers.LSTM(64)(x)\n",
    "#     x = keras.layers.Dense(64)(x)\n",
    "#     x = keras.layers.BatchNormalization()(x)\n",
    "#     x = keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "#     # Flatten and classify real vs fake\n",
    "#     x = keras.layers.Flatten()(x)\n",
    "#     x = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "#     model = keras.models.Model(inputs, x, name=\"Discriminator\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define loss functions\n",
    "# def generator_loss(fake_output, reconstructed_input, real_latent, reconstructed_latent, lambdas):\n",
    "#     bce_loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "#     # Reconstruction loss (input space, L1 loss)\n",
    "#     lx = tf.reduce_mean(tf.abs(reconstructed_input - real_latent))\n",
    "    \n",
    "#     # Latent space reconstruction loss (L2 loss)\n",
    "#     lz = tf.reduce_mean(tf.square(reconstructed_latent - real_latent))\n",
    "    \n",
    "#     # Adversarial loss (Binary Crossentropy)\n",
    "#     adversarial_loss = bce_loss(tf.ones_like(fake_output), fake_output)\n",
    "    \n",
    "#     # Combine losses\n",
    "#     loss = lambdas[0] * lx + lambdas[1] * lz + lambdas[2] * adversarial_loss\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# def discriminator_loss(real_output, fake_output):\n",
    "#     bce_loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "#     # Binary crossentropy loss for real and fake samples\n",
    "#     real_loss = bce_loss(tf.ones_like(real_output), real_output)\n",
    "#     fake_loss = bce_loss(tf.zeros_like(fake_output), fake_output)\n",
    "    \n",
    "#     # Total discriminator loss\n",
    "#     return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build and compile the models\n",
    "# generator = build_generator()\n",
    "# discriminator = build_discriminator()\n",
    "\n",
    "# # Optimizers\n",
    "# gen_optimizer = keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
    "# disc_optimizer = keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
    "\n",
    "# # Placeholders for real and fake inputs\n",
    "# real_data = keras.layers.Input(shape=(time_steps, features))\n",
    "# fake_data = generator(real_data)\n",
    "\n",
    "# # Discriminator training\n",
    "# real_output = discriminator(real_data)\n",
    "# fake_output = discriminator(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def train_step(real_data):\n",
    "#     # Latent vectors for reconstruction\n",
    "#     real_latent = generator(real_data)\n",
    "    \n",
    "#     with tf.GradientTape(persistent=True) as tape:\n",
    "#         # Generate fake data\n",
    "#         fake_data = generator(real_data)\n",
    "        \n",
    "#         # Discriminator outputs\n",
    "#         real_output = discriminator(real_data)\n",
    "#         fake_output = discriminator(fake_data)\n",
    "        \n",
    "#         # Latent space reconstruction\n",
    "#         reconstructed_latent = generator(fake_data)\n",
    "        \n",
    "#         # Calculate losses\n",
    "#         gen_loss = generator_loss(fake_output, fake_data, real_latent, reconstructed_latent, lambdas=[0.1, 0.1, 0.8])\n",
    "#         disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "#     # Update gradients\n",
    "#     gradients_of_generator = tape.gradient(gen_loss, generator.trainable_variables)\n",
    "#     gradients_of_discriminator = tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "#     gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "#     disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "#     return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# def train(dataset, epochs):\n",
    "#     for epoch in range(epochs):\n",
    "#         for real_data in dataset:\n",
    "#             gen_loss, disc_loss = train_step(real_data)\n",
    "        \n",
    "#         print(f'Epoch {epoch + 1}, Gen Loss: {gen_loss}, Disc Loss: {disc_loss}')\n",
    "\n",
    "# # Anomaly score and labeling\n",
    "# def compute_anomaly_score(real_data):\n",
    "#     fake_data = generator.predict(real_data)\n",
    "    \n",
    "#     # Latent space reconstruction loss\n",
    "#     latent_real = generator(real_data)\n",
    "#     latent_fake = generator(fake_data)\n",
    "    \n",
    "#     anomaly_score = np.mean(np.square(latent_real - latent_fake), axis=1)\n",
    "    \n",
    "#     return anomaly_score\n",
    "\n",
    "# def label_data(anomaly_score, thresholds):\n",
    "#     labels = np.zeros_like(anomaly_score)\n",
    "#     labels[anomaly_score < thresholds[0]] = 0  # \"Good\"\n",
    "#     labels[(anomaly_score >= thresholds[0]) & (anomaly_score < thresholds[1])] = 1  # \"Watching\"\n",
    "#     labels[(anomaly_score >= thresholds[1]) & (anomaly_score < thresholds[2])] = 2  # \"Warning\"\n",
    "#     labels[anomaly_score >= thresholds[2]] = 3  # \"Fault\"\n",
    "#     return labels\n",
    "\n",
    "# # Model Evaluation\n",
    "# def evaluate_model(real_data, true_labels, thresholds):\n",
    "#     # Compute anomaly scores\n",
    "#     anomaly_scores = compute_anomaly_score(real_data)\n",
    "    \n",
    "#     # Label data based on thresholds\n",
    "#     predicted_labels = label_data(anomaly_scores, thresholds)\n",
    "    \n",
    "#     # Calculate evaluation metrics\n",
    "#     precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "#     recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "#     f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "#     cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "#     print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample dataset and training example\n",
    "# train_data = np.random.randn(1000, time_steps, features)\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\farra\\anaconda3\\lib\\site-packages\\keras\\backend.py:5673: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gen Loss: 0.5941934585571289, Disc Loss: 1.27066969871521\n",
      "Epoch 2, Gen Loss: 2.3630566596984863, Disc Loss: 1.4189093112945557\n",
      "Epoch 3, Gen Loss: 1.8198363780975342, Disc Loss: 1.0383702516555786\n",
      "Epoch 4, Gen Loss: 1.8360899686813354, Disc Loss: 0.5423786044120789\n",
      "Epoch 5, Gen Loss: 2.1641626358032227, Disc Loss: 0.2957163453102112\n",
      "Epoch 6, Gen Loss: 2.6850733757019043, Disc Loss: 0.15120694041252136\n",
      "Epoch 7, Gen Loss: 0.7025512456893921, Disc Loss: 1.4290196895599365\n",
      "Epoch 8, Gen Loss: 1.349381446838379, Disc Loss: 0.2759851813316345\n",
      "Epoch 9, Gen Loss: 0.18524624407291412, Disc Loss: 1.8066127300262451\n",
      "Epoch 10, Gen Loss: 0.5883899331092834, Disc Loss: 1.359813928604126\n",
      "Epoch 11, Gen Loss: 0.6414827704429626, Disc Loss: 1.188891887664795\n",
      "Epoch 12, Gen Loss: 0.6848341226577759, Disc Loss: 1.198060393333435\n",
      "Epoch 13, Gen Loss: 0.6785936951637268, Disc Loss: 1.2069724798202515\n",
      "Epoch 14, Gen Loss: 0.70448899269104, Disc Loss: 1.21113121509552\n",
      "Epoch 15, Gen Loss: 0.7787841558456421, Disc Loss: 1.2198164463043213\n",
      "Epoch 16, Gen Loss: 0.698591947555542, Disc Loss: 1.2999980449676514\n",
      "Epoch 17, Gen Loss: 0.6945374011993408, Disc Loss: 1.2800188064575195\n",
      "Epoch 18, Gen Loss: 0.6963999271392822, Disc Loss: 1.2897343635559082\n",
      "Epoch 19, Gen Loss: 0.7223489880561829, Disc Loss: 1.3070944547653198\n",
      "Epoch 20, Gen Loss: 0.7170488834381104, Disc Loss: 1.3025277853012085\n",
      "Epoch 21, Gen Loss: 0.7342883348464966, Disc Loss: 1.2709879875183105\n",
      "Epoch 22, Gen Loss: 0.697924017906189, Disc Loss: 1.413387656211853\n",
      "Epoch 23, Gen Loss: 0.871562123298645, Disc Loss: 1.2329649925231934\n",
      "Epoch 24, Gen Loss: 0.8700152635574341, Disc Loss: 1.2321374416351318\n",
      "Epoch 25, Gen Loss: 0.8677889704704285, Disc Loss: 1.2011456489562988\n",
      "Epoch 26, Gen Loss: 0.8672682046890259, Disc Loss: 1.1328799724578857\n",
      "Epoch 27, Gen Loss: 0.8479743599891663, Disc Loss: 1.1129164695739746\n",
      "Epoch 28, Gen Loss: 0.8626324534416199, Disc Loss: 1.2336254119873047\n",
      "Epoch 29, Gen Loss: 0.8708310723304749, Disc Loss: 1.1880173683166504\n",
      "Epoch 30, Gen Loss: 0.867107093334198, Disc Loss: 1.225809097290039\n",
      "Epoch 31, Gen Loss: 0.8604442477226257, Disc Loss: 1.248150110244751\n",
      "Epoch 32, Gen Loss: 0.8750394582748413, Disc Loss: 1.2502033710479736\n",
      "Epoch 33, Gen Loss: 0.8801223635673523, Disc Loss: 1.2197715044021606\n",
      "Epoch 34, Gen Loss: 0.8753499388694763, Disc Loss: 1.2408735752105713\n",
      "Epoch 35, Gen Loss: 0.8622724413871765, Disc Loss: 1.237159252166748\n",
      "Epoch 36, Gen Loss: 0.8652657866477966, Disc Loss: 1.2320888042449951\n",
      "Epoch 37, Gen Loss: 0.8667182326316833, Disc Loss: 1.2307618856430054\n",
      "Epoch 38, Gen Loss: 0.8668010830879211, Disc Loss: 1.2302430868148804\n",
      "Epoch 39, Gen Loss: 0.8681274652481079, Disc Loss: 1.2287232875823975\n",
      "Epoch 40, Gen Loss: 0.8647217750549316, Disc Loss: 1.2142608165740967\n",
      "Epoch 41, Gen Loss: 0.871526837348938, Disc Loss: 1.2126693725585938\n",
      "Epoch 42, Gen Loss: 0.8658357858657837, Disc Loss: 1.2123019695281982\n",
      "Epoch 43, Gen Loss: 0.8728271722793579, Disc Loss: 1.2297464609146118\n",
      "Epoch 44, Gen Loss: 0.8416635990142822, Disc Loss: 1.2308331727981567\n",
      "Epoch 45, Gen Loss: 0.8700297474861145, Disc Loss: 1.197718858718872\n",
      "Epoch 46, Gen Loss: 0.8558234572410583, Disc Loss: 1.2454310655593872\n",
      "Epoch 47, Gen Loss: 0.8849721550941467, Disc Loss: 1.1996324062347412\n",
      "Epoch 48, Gen Loss: 0.8653106689453125, Disc Loss: 1.2341632843017578\n",
      "Epoch 49, Gen Loss: 0.8679419159889221, Disc Loss: 1.2308738231658936\n",
      "Epoch 50, Gen Loss: 0.8567250370979309, Disc Loss: 1.2314255237579346\n"
     ]
    }
   ],
   "source": [
    "# # Train the model\n",
    "# train(train_dataset, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "# fake_data = generator.predict(train_data)\n",
    "\n",
    "# # Latent space reconstruction loss\n",
    "# latent_real = generator(train_data)\n",
    "# latent_fake = generator(fake_data)\n",
    "\n",
    "# anomaly_score = np.mean(np.square(latent_real - latent_fake), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Evaluate the model (replace `true_labels` with actual labels)\n",
    "# true_labels = np.random.randint(0, 4, size=1000)\n",
    "# evaluate_model(train_data, true_labels, thresholds=[0.2, 0.5, 0.7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
