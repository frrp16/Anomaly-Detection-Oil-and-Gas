{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Conv1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- TadGAN Components --\n",
    "\n",
    "# Generator (Encoder)\n",
    "def build_encoder(input_shape, latent_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(100, return_sequences=True)(inputs)\n",
    "    x = LSTM(latent_dim)(x)  # Latent space representation\n",
    "    model = Model(inputs, x, name='encoder')\n",
    "    return model\n",
    "\n",
    "# Generator (Decoder)\n",
    "def build_decoder(latent_dim, output_shape):\n",
    "    inputs = Input(shape=(latent_dim,))\n",
    "    x = Dense(output_shape[0])(inputs)\n",
    "    x = LSTM(100, return_sequences=True)(tf.expand_dims(x, -1))\n",
    "    outputs = LSTM(output_shape[1], return_sequences=True)(x)\n",
    "    model = Model(inputs, outputs, name='decoder')\n",
    "    return model\n",
    "\n",
    "# Critic (for time series)\n",
    "def build_critic(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    model = Model(inputs, outputs, name='critic')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Loss Functions --\n",
    "\n",
    "# Wasserstein loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "# Cycle consistency loss (L2 norm)\n",
    "def cycle_consistency_loss(real, reconstructed):\n",
    "    return tf.reduce_mean(tf.square(real - reconstructed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- TadGAN Model --\n",
    "\n",
    "class TadGAN:\n",
    "    def __init__(self, input_shape, latent_dim):\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Build Generator (Encoder and Decoder)\n",
    "        self.encoder = build_encoder(input_shape, latent_dim)\n",
    "        self.decoder = build_decoder(latent_dim, input_shape)\n",
    "        \n",
    "        # Build Critics\n",
    "        self.critic_x = build_critic(input_shape)\n",
    "        self.critic_z = build_critic((latent_dim,1))\n",
    "\n",
    "        # Optimizers\n",
    "        self.critic_optimizer = Adam(learning_rate=1e-4)\n",
    "        self.generator_optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "    # Training step\n",
    "    def train_step(self, real_data):\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Forward pass through the encoder and decoder (Generator)\n",
    "            encoded = self.encoder(real_data)\n",
    "            reconstructed = self.decoder(encoded)\n",
    "            \n",
    "            # Forward pass through the critics\n",
    "            fake_data = self.decoder(random_latent_vectors)\n",
    "            critic_real_x = self.critic_x(real_data)\n",
    "            critic_fake_x = self.critic_x(fake_data)\n",
    "            critic_real_z = self.critic_z(random_latent_vectors)\n",
    "            critic_fake_z = self.critic_z(encoded)\n",
    "            \n",
    "            # Compute losses\n",
    "            cycle_loss = cycle_consistency_loss(real_data, reconstructed)\n",
    "            critic_loss_x = wasserstein_loss(tf.ones_like(critic_real_x), critic_real_x) + \\\n",
    "                            wasserstein_loss(-tf.ones_like(critic_fake_x), critic_fake_x)\n",
    "            critic_loss_z = wasserstein_loss(tf.ones_like(critic_real_z), critic_real_z) + \\\n",
    "                            wasserstein_loss(-tf.ones_like(critic_fake_z), critic_fake_z)\n",
    "            generator_loss = -wasserstein_loss(tf.ones_like(critic_fake_x), critic_fake_x) - \\\n",
    "                             wasserstein_loss(tf.ones_like(critic_fake_z), critic_fake_z) + cycle_loss\n",
    "\n",
    "        # Apply gradients to the critics\n",
    "        critic_grads_x = tape.gradient(critic_loss_x, self.critic_x.trainable_weights)\n",
    "        critic_grads_z = tape.gradient(critic_loss_z, self.critic_z.trainable_weights)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads_x, self.critic_x.trainable_weights))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads_z, self.critic_z.trainable_weights))\n",
    "        \n",
    "        # Apply gradients to the generator\n",
    "        generator_grads = tape.gradient(generator_loss, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
    "        self.generator_optimizer.apply_gradients(zip(generator_grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
    "        \n",
    "        return {\"critic_loss_x\": critic_loss_x, \"critic_loss_z\": critic_loss_z, \"generator_loss\": generator_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 100, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100, 1), dtype=tf.float32, name='input_11'), name='input_11', description=\"created by layer 'input_11'\"), but it was called on an input with incompatible shape (1000, 100, 100).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\farra\\AppData\\Local\\Temp\\ipykernel_9936\\2502222530.py\", line 21, in training_step  *\n        return tadgan.train_step(real_data)\n    File \"C:\\Users\\farra\\AppData\\Local\\Temp\\ipykernel_9936\\341070802.py\", line 27, in train_step  *\n        encoded = self.encoder(real_data)\n    File \"c:\\Users\\farra\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\farra\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"encoder\" \"                 f\"(type Functional).\n    \n    Input 0 of layer \"lstm_11\" is incompatible with the layer: expected shape=(None, None, 1), found shape=(1000, 100, 100)\n    \n    Call arguments received by layer \"encoder\" \"                 f\"(type Functional):\n      • inputs=tf.Tensor(shape=(1000, 100, 100), dtype=float64)\n      • training=None\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 26\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\farra\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemm3t7uwc.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__training_step\u001b[1;34m(real_data)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tadgan)\u001b[38;5;241m.\u001b[39mtrain_step, (ag__\u001b[38;5;241m.\u001b[39mld(real_data),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileu1rj9c9v.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(self, real_data)\u001b[0m\n\u001b[0;32m     11\u001b[0m random_latent_vectors \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal, (), \u001b[38;5;28mdict\u001b[39m(shape\u001b[38;5;241m=\u001b[39m(ag__\u001b[38;5;241m.\u001b[39mld(batch_size), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mlatent_dim)), fscope)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape(persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 13\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     reconstructed \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdecoder, (ag__\u001b[38;5;241m.\u001b[39mld(encoded),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     15\u001b[0m     fake_data \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdecoder, (ag__\u001b[38;5;241m.\u001b[39mld(random_latent_vectors),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mc:\\Users\\farra\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\farra\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py:295\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    297\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\farra\\AppData\\Local\\Temp\\ipykernel_9936\\2502222530.py\", line 21, in training_step  *\n        return tadgan.train_step(real_data)\n    File \"C:\\Users\\farra\\AppData\\Local\\Temp\\ipykernel_9936\\341070802.py\", line 27, in train_step  *\n        encoded = self.encoder(real_data)\n    File \"c:\\Users\\farra\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\farra\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"encoder\" \"                 f\"(type Functional).\n    \n    Input 0 of layer \"lstm_11\" is incompatible with the layer: expected shape=(None, None, 1), found shape=(1000, 100, 100)\n    \n    Call arguments received by layer \"encoder\" \"                 f\"(type Functional):\n      • inputs=tf.Tensor(shape=(1000, 100, 100), dtype=float64)\n      • training=None\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -- Data Preparation and Training Example --\n",
    "\n",
    "# Generate synthetic data for time series (e.g., sine wave data)\n",
    "def generate_synthetic_data(samples, timesteps, features):\n",
    "    t = np.linspace(0, 100, timesteps)\n",
    "    data = np.sin(t) + 0.1 * np.random.randn(samples, timesteps, features)\n",
    "    return data\n",
    "\n",
    "# Instantiate the model\n",
    "input_shape = (100, 1)  # 100 timesteps, 1 feature\n",
    "latent_dim = 20  # Latent space dimension\n",
    "\n",
    "tadgan = TadGAN(input_shape, latent_dim)\n",
    "\n",
    "# Generate synthetic training data\n",
    "X_train = generate_synthetic_data(1000, 100, 1)\n",
    "\n",
    "# Compile training step into a TensorFlow function\n",
    "@tf.function\n",
    "def training_step(real_data):\n",
    "    return tadgan.train_step(real_data)\n",
    "\n",
    "# Train the model for some epochs\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    losses = training_step(X_train)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: {losses}\")\n",
    "\n",
    "# -- Testing Example --\n",
    "\n",
    "# Test the trained model on new data\n",
    "X_test = generate_synthetic_data(10, 100, 1)\n",
    "encoded_test = tadgan.encoder(X_test)\n",
    "reconstructed_test = tadgan.decoder(encoded_test)\n",
    "\n",
    "# Compute reconstruction error\n",
    "reconstruction_error = np.mean(np.abs(X_test - reconstructed_test), axis=(1, 2))\n",
    "print(\"Reconstruction Error:\", reconstruction_error)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
