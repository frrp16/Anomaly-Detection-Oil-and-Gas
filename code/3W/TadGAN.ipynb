{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVhHscIMEtMz"
   },
   "source": [
    "### TadGAN for Tensorflow 2.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvvVAq07YKLP"
   },
   "source": [
    "#### Part 1\n",
    "- Connect and authenticate user google drive \n",
    "- Data load and prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_TAcBTAVPOj",
    "outputId": "b3e9e90b-7395-42db-a1ce-89e929794f7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# # drive mount \n",
    "# from google.colab import drive\n",
    "# import os\n",
    "# drive.mount('/content/drive')#, force_remount=True)  # Force_remount 는 강제적으로 해당 경로로 mount 하겠다는 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O8C3rTcZVgzM"
   },
   "outputs": [],
   "source": [
    "# os.chdir('/content/drive/My Drive/CoLab/TimeSeries/TadGAN') # 다음 python 실행 부터는 해당 코드만 실행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "R71s6TR4U72u"
   },
   "outputs": [],
   "source": [
    "# load generals\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import time\n",
    "\n",
    "# from utils import plot, plot_ts, plot_rws, plot_error, unroll_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XKNr2O4aYT5m"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('nyc_taxi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "EBg4-_DoYgwG",
    "outputId": "ee3d0355-ebff-4040-e67e-ebcb542a8937"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10320, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "npWtZAD5X-Hi"
   },
   "outputs": [],
   "source": [
    "def time_segments_aggregate(X, interval, time_column, method=['mean']):\n",
    "    \"\"\"Aggregate values over given time span.\n",
    "    Args:\n",
    "        X (ndarray or pandas.DataFrame):\n",
    "            N-dimensional sequence of values.\n",
    "        interval (int):\n",
    "            Integer denoting time span to compute aggregation of.\n",
    "        time_column (int):\n",
    "            Column of X that contains time values.\n",
    "        method (str or list):\n",
    "            Optional. String describing aggregation method or list of strings describing multiple\n",
    "            aggregation methods. If not given, `mean` is used.\n",
    "    Returns:\n",
    "        ndarray, ndarray:\n",
    "            * Sequence of aggregated values, one column for each aggregation method.\n",
    "            * Sequence of index values (first index of each aggregated segment).\n",
    "    \"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    X = X.sort_values(time_column).set_index(time_column)\n",
    "\n",
    "    if isinstance(method, str):\n",
    "        method = [method]\n",
    "\n",
    "    start_ts = X.index.values[0]\n",
    "    max_ts = X.index.values[-1]\n",
    "\n",
    "    values = list()\n",
    "    index = list()\n",
    "    while start_ts <= max_ts:\n",
    "        end_ts = start_ts + interval\n",
    "        subset = X.loc[start_ts:end_ts - 1]\n",
    "        aggregated = [\n",
    "            getattr(subset, agg)(skipna=True).values\n",
    "            for agg in method\n",
    "        ]\n",
    "        values.append(np.concatenate(aggregated))\n",
    "        index.append(start_ts)\n",
    "        start_ts = end_ts\n",
    "\n",
    "    return np.asarray(values), np.asarray(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bfQE_IlmZv4P"
   },
   "outputs": [],
   "source": [
    "# TimeSegments \n",
    "X, index = time_segments_aggregate(df, interval=1800, time_column='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "49rVid5gYqAI"
   },
   "outputs": [],
   "source": [
    "imp = SimpleImputer()\n",
    "X = imp.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "q15IDpMUYyel"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YuN0SmzCZ4gx"
   },
   "outputs": [],
   "source": [
    "def rolling_window_sequences(X, index, window_size, target_size, step_size, target_column,\n",
    "                             drop=None, drop_windows=False):\n",
    "    \"\"\"Create rolling window sequences out of time series data.\n",
    "    The function creates an array of input sequences and an array of target sequences by rolling\n",
    "    over the input sequence with a specified window.\n",
    "    Optionally, certain values can be dropped from the sequences.\n",
    "    Args:\n",
    "        X (ndarray):\n",
    "            N-dimensional sequence to iterate over.\n",
    "        index (ndarray):\n",
    "            Array containing the index values of X.\n",
    "        window_size (int):\n",
    "            Length of the input sequences.\n",
    "        target_size (int):\n",
    "            Length of the target sequences.\n",
    "        step_size (int):\n",
    "            Indicating the number of steps to move the window forward each round.\n",
    "        target_column (int):\n",
    "            Indicating which column of X is the target.\n",
    "        drop (ndarray or None or str or float or bool):\n",
    "            Optional. Array of boolean values indicating which values of X are invalid, or value\n",
    "            indicating which value should be dropped. If not given, `None` is used.\n",
    "        drop_windows (bool):\n",
    "            Optional. Indicates whether the dropping functionality should be enabled. If not\n",
    "            given, `False` is used.\n",
    "    Returns:\n",
    "        ndarray, ndarray, ndarray, ndarray:\n",
    "            * input sequences.\n",
    "            * target sequences.\n",
    "            * first index value of each input sequence.\n",
    "            * first index value of each target sequence.\n",
    "    \"\"\"\n",
    "    out_X = list()\n",
    "    out_y = list()\n",
    "    X_index = list()\n",
    "    y_index = list()\n",
    "    target = X[:, target_column]\n",
    "\n",
    "    if drop_windows:\n",
    "        if hasattr(drop, '__len__') and (not isinstance(drop, str)):\n",
    "            if len(drop) != len(X):\n",
    "                raise Exception('Arrays `drop` and `X` must be of the same length.')\n",
    "        else:\n",
    "            if isinstance(drop, float) and np.isnan(drop):\n",
    "                drop = np.isnan(X)\n",
    "            else:\n",
    "                drop = X == drop\n",
    "\n",
    "    start = 0\n",
    "    max_start = len(X) - window_size - target_size + 1\n",
    "    while start < max_start:\n",
    "        end = start + window_size\n",
    "\n",
    "        if drop_windows:\n",
    "            drop_window = drop[start:end + target_size]\n",
    "            to_drop = np.where(drop_window)[0]\n",
    "            if to_drop.size:\n",
    "                start += to_drop[-1] + 1\n",
    "                continue\n",
    "\n",
    "        out_X.append(X[start:end])\n",
    "        out_y.append(target[end:end + target_size])\n",
    "        X_index.append(index[start])\n",
    "        y_index.append(index[end])\n",
    "        start = start + step_size\n",
    "\n",
    "    return np.asarray(out_X), np.asarray(out_y), np.asarray(X_index), np.asarray(y_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "V3oekRlSZ6Wn"
   },
   "outputs": [],
   "source": [
    "X, y, X_index, y_index = rolling_window_sequences(X, index, \n",
    "                                                  window_size=100, \n",
    "                                                  target_size=1, \n",
    "                                                  step_size=1,\n",
    "                                                  target_column=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhYaWBwpZ-Az",
    "outputId": "0eb039de-06d2-4b55-97fa-f3ca882c56c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data input shape: (10222, 100, 1)\n",
      "Training data index shape: (10222,)\n",
      "Training y shape: (10222, 1)\n",
      "Training y index shape: (10222,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data input shape: {}\".format(X.shape))\n",
    "print(\"Training data index shape: {}\".format(X_index.shape))\n",
    "print(\"Training y shape: {}\".format(y.shape))\n",
    "print(\"Training y index shape: {}\".format(y_index.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjqfKEg9aApL"
   },
   "source": [
    "#### Part 2 \n",
    "\n",
    "- GPU check for TadGAN \n",
    "- Load Tensorflow, Keras, Layers .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTYvrfHTRUbf",
    "outputId": "c16c1127-6760-4c75-c981-54449c95cd60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Check gpu envrionmental \n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import math\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU') \n",
    "if gpus: \n",
    "    try: \n",
    "        for gpu in gpus: \n",
    "            tf.config.experimental.set_memory_growth(gpu, True) \n",
    "    except RuntimeError as e: \n",
    "        print(e)\n",
    "print (gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "o8ALUCPXEtM-"
   },
   "outputs": [],
   "source": [
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7ys30Y68EtM8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import similaritymeasures as sm\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Flatten, Dense, Reshape, UpSampling1D, TimeDistributed\n",
    "from tensorflow.keras.layers import Activation, Conv1D, LeakyReLU, Dropout, Add, Layer\n",
    "# from tensorflow.compat.v1.keras.layers import CuDNNLSTM as CUDNNLSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from functools import partial\n",
    "from scipy import integrate, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "coQevcffOvWV"
   },
   "outputs": [],
   "source": [
    "# Model Building 개별 함수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uj_WD27AOvWV"
   },
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(Layer):\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((64, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fDOTybi0OvWV"
   },
   "outputs": [],
   "source": [
    "def build_encoder_layer(input_shape, encoder_reshape_shape):    \n",
    "    input_layer = layers.Input(shape=input_shape)    \n",
    "    x = layers.Bidirectional(LSTM(units=100, return_sequences=True))(input_layer)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(20)(x)\n",
    "    x = layers.Reshape(target_shape=encoder_reshape_shape)(x)\n",
    "    model = keras.models.Model(input_layer, x, name='encoder')\n",
    "    return model\n",
    "\n",
    "def build_generator_layer(input_shape, generator_reshape_shape):\n",
    "    \n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Flatten()(input_layer)\n",
    "    x = layers.Dense(generator_reshape_shape[0])(x)\n",
    "    x = layers.Reshape(target_shape=generator_reshape_shape)(x)\n",
    "    x = layers.Bidirectional(LSTM(units=64, return_sequences=True), merge_mode='concat')(x)\n",
    "    x = layers.UpSampling1D(size=2)(x)\n",
    "    x = layers.Bidirectional(LSTM(units=64, return_sequences=True), merge_mode='concat')(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(1))(x)\n",
    "    x = layers.Activation(activation='tanh')(x)\n",
    "    model = keras.models.Model(input_layer, x, name='generator')\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "def build_critic_x_layer(input_shape):\n",
    "    \n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv1D(filters=64, kernel_size=5)(input_layer)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.25)(x)\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.25)(x)\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.25)(x)\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.25)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(units=1)(x)\n",
    "    model = keras.models.Model(input_layer, x, name='critic_x')\n",
    "    \n",
    "    return model \n",
    "\n",
    "\n",
    "def build_critic_z_layer(input_shape):\n",
    "    \n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Flatten()(input_layer)\n",
    "    x = layers.Dense(units=100)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.2)(x)    \n",
    "    x = layers.Dense(units=100)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(rate=0.2)(x)  \n",
    "    x = layers.Dense(units=1)(x)\n",
    "    model = keras.models.Model(input_layer, x, name='critic_z')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OCdHMC3BOvWW"
   },
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "#    return tf.reduce_mean(y_true * y_pred)\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "r_kqDLYdx33S"
   },
   "outputs": [],
   "source": [
    "window_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "z8ZcWpI7OvWW"
   },
   "outputs": [],
   "source": [
    "# Layer Parameters\n",
    "encoder_input_shape = (window_size, 1)\n",
    "generator_input_shape = (20, 1)\n",
    "\n",
    "critic_x_input_shape = (window_size, 1)\n",
    "critic_z_input_shape = (20,1)\n",
    "\n",
    "encoder_reshape_shape = (20, 1)\n",
    "generator_reshape_shape = (window_size//2, 1) # window_size//3 <- 3 is Upsampling size\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Build Model\n",
    "encoder = build_encoder_layer(input_shape=encoder_input_shape,\n",
    "                              encoder_reshape_shape=encoder_reshape_shape)\n",
    "\n",
    "generator = build_generator_layer(input_shape=generator_input_shape,\n",
    "                                  generator_reshape_shape=generator_reshape_shape)\n",
    "\n",
    "critic_x = build_critic_x_layer(input_shape=critic_x_input_shape)\n",
    "critic_z = build_critic_z_layer(input_shape=critic_z_input_shape)\n",
    "\n",
    "encoder_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "critic_x_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "critic_z_optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BmoAtlpMyEJM",
    "outputId": "359d9755-29c5-41ab-bfc1-24cf308467fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 1)]          0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100, 200)         81600     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 20000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                400020    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 20, 1)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 481,620\n",
      "Trainable params: 481,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0OR4I1CyHQP",
    "outputId": "027a4b22-a917-45ff-9fdf-c7288b761204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                1050      \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 50, 1)             0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 50, 128)          33792     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " up_sampling1d (UpSampling1D  (None, 100, 128)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 100, 128)         98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 100, 1)           129       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100, 1)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 133,787\n",
      "Trainable params: 133,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfyoOr23yKx_",
    "outputId": "d6d840e8-9858-4c08-d38c-a1e15f0ce2f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"critic_x\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 100, 1)]          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 96, 64)            384       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 96, 64)            0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 96, 64)            0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 92, 64)            20544     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 92, 64)            0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 92, 64)            0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 88, 64)            20544     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 88, 64)            0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 88, 64)            0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 84, 64)            20544     \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 84, 64)            0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 84, 64)            0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 5376)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 5377      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,393\n",
      "Trainable params: 67,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_x.summary() # Original input X 에 대한 감시 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bpydPLSHyNkI",
    "outputId": "9471b900-1122-4224-c205-5356eba31a8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"critic_z\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               2100      \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,301\n",
      "Trainable params: 12,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_z.summary()  # Generated 되는 것에 대한 감시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "g89LOVyhyQGF"
   },
   "outputs": [],
   "source": [
    "latent_dim = 20\n",
    "shape = (window_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2EPMhkZLOvWW"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def critic_x_train_on_batch(x, z):\n",
    "    # Loss 크게 이상 없음 \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        valid_x = critic_x(x)\n",
    "        x_ = generator(z)\n",
    "        fake_x = critic_x(x_)\n",
    "        \n",
    "        # Interpolated \n",
    "        alpha = tf.random.uniform([batch_size, 1, 1], 0.0, 1.0)\n",
    "        interpolated = alpha * x + (1 - alpha) * x_ \n",
    "        \n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            pred = critic_x(interpolated)\n",
    "        \n",
    "        grads = gp_tape.gradient(pred, interpolated)\n",
    "        grad_norm = tf.norm(tf.reshape(grads, (batch_size, -1)), axis=1)\n",
    "        gp_loss = 10.0*tf.reduce_mean(tf.square(grad_norm - 1.))\n",
    "#         grads = tf.square(grads)\n",
    "#         ddx = tf.sqrt(tf.reduce_sum(grads, axis=np.arange(1, len(grads.shape))))\n",
    "#        gp_loss = tf.reduce_mean((1.0 - ddx) ** 2)\n",
    "                \n",
    "        loss1 = wasserstein_loss(-tf.ones_like(valid_x), valid_x)\n",
    "        loss2 = wasserstein_loss(tf.ones_like(fake_x), fake_x)\n",
    "        #loss = tf.add_n([loss1, loss2, gp_loss*10.0])        \n",
    "        loss = loss1 + loss2 + gp_loss\n",
    "#        loss = tf.reduce_mean(loss)\n",
    "                        \n",
    "    gradients = tape.gradient(loss, critic_x.trainable_weights)\n",
    "    critic_x_optimizer.apply_gradients(zip(gradients, critic_x.trainable_weights))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ZTJplgZCyXVc"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def critic_z_train_on_batch(x, z):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z_ = encoder(x)   \n",
    "        valid_z = critic_z(z)             \n",
    "        fake_z = critic_z(z_) # <- critic_z 의 결과가 매우 않음 \n",
    "        \n",
    "        # Interpolated \n",
    "        alpha = tf.random.uniform([batch_size, 1, 1], 0.0, 1.0)\n",
    "        interpolated = alpha * z + (1 - alpha) * z_ \n",
    "                \n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            pred = critic_z(interpolated, training=True)\n",
    "            \n",
    "        grads = gp_tape.gradient(pred, interpolated)\n",
    "        grad_norm = tf.norm(tf.reshape(grads, (batch_size, -1)), axis=1)\n",
    "        gp_loss = 10.0*tf.reduce_mean(tf.square(grad_norm - 1.))\n",
    "\n",
    "#         grads = tf.square(grads)\n",
    "#         ddx = tf.sqrt(tf.reduce_sum(grads, axis=np.arange(1, len(grads.shape))))\n",
    "#         gp_loss = tf.reduce_mean((1.0 - ddx) ** 2)\n",
    "        \n",
    "        loss1 = wasserstein_loss(-tf.ones_like(valid_z), valid_z)\n",
    "        loss2 = wasserstein_loss(tf.ones_like(fake_z), fake_z) # <- 이게 미친듯이 뜀. \n",
    "        loss = loss1 + loss2 + gp_loss\n",
    "#        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "    gradients = tape.gradient(loss, critic_z.trainable_weights)\n",
    "    critic_z_optimizer.apply_gradients(zip(gradients, critic_z.trainable_weights))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9YLCQG7VyadM"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def enc_gen_train_on_batch(x, z):\n",
    "    with tf.GradientTape() as enc_tape:\n",
    "        \n",
    "        z_gen_ = encoder(x, training=True)\n",
    "        x_gen_ = generator(z, training=False)        \n",
    "        x_gen_rec = generator(z_gen_, training=False)\n",
    "        \n",
    "        fake_gen_x = critic_x(x_gen_, training=False)\n",
    "        fake_gen_z = critic_z(z_gen_, training=False)\n",
    "        \n",
    "        loss1 = wasserstein_loss(fake_gen_x, -tf.ones_like(fake_gen_x))\n",
    "        loss2 = wasserstein_loss(fake_gen_z, -tf.ones_like(fake_gen_z))\n",
    "        loss3 = 10.0*tf.reduce_mean(tf.keras.losses.MSE(x, x_gen_rec))\n",
    "\n",
    "        enc_loss = loss1 + loss2 + loss3\n",
    "        \n",
    "    gradients_encoder = enc_tape.gradient(enc_loss, encoder.trainable_weights)\n",
    "    encoder_optimizer.apply_gradients(zip(gradients_encoder, encoder.trainable_weights))\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        \n",
    "        z_gen_ = encoder(x, training=False)\n",
    "        x_gen_ = generator(z, training=True)        \n",
    "        x_gen_rec = generator(z_gen_, training=True)\n",
    "        \n",
    "        fake_gen_x = critic_x(x_gen_, training=False)\n",
    "        fake_gen_z = critic_z(z_gen_, training=False)\n",
    "        \n",
    "        loss1 = wasserstein_loss(fake_gen_x, -tf.ones_like(fake_gen_x))\n",
    "        loss2 = wasserstein_loss(fake_gen_z, -tf.ones_like(fake_gen_z))\n",
    "        loss3 = 10.0*tf.reduce_mean(tf.keras.losses.MSE(x, x_gen_rec))\n",
    "\n",
    "        gen_loss = loss1 + loss2 + loss3\n",
    "        \n",
    "    gradients_generator = gen_tape.gradient(gen_loss, generator.trainable_weights)    \n",
    "    generator_optimizer.apply_gradients(zip(gradients_generator, generator.trainable_weights))    \n",
    "    return enc_loss, gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Fd-KP-prOvWX"
   },
   "outputs": [],
   "source": [
    "# Train parameters\n",
    "batch_size = 64\n",
    "n_critics = 5\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BskyRITqyeng",
    "outputId": "2c199183-7473-44f6-9efc-eb3231d4b014"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10222, 100, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21-Jg2_6yhKg",
    "outputId": "fc364882-0f4d-4150-ed41-cd1ec72277da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\farra\\AppData\\Local\\Temp\\ipykernel_11516\\2874510872.py:1: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    }
   ],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "43QYfgRCOvWX"
   },
   "outputs": [],
   "source": [
    "# Data Reshape\n",
    "X = X.reshape((-1, shape[0], 1))\n",
    "X_ = np.copy(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pp7JdciKOvWX",
    "outputId": "6928452a-c36b-4233-ff12-42b9a97f48ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000, [Dx loss: -1.0165661573410034] [Dz loss: -2.4103171825408936] [E loss: 2.858992576599121] [G loss: 2.0659515857696533]\n",
      "Epoch: 2/1000, [Dx loss: -1.1780644655227661] [Dz loss: -2.1747219562530518] [E loss: -0.1632768213748932] [G loss: -0.6455420255661011]\n",
      "Epoch: 3/1000, [Dx loss: -1.0964525938034058] [Dz loss: -2.1285815238952637] [E loss: -0.052065398544073105] [G loss: -0.40775078535079956]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m encoder\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     39\u001b[0m generator\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m enc_loss, gen_loss \u001b[38;5;241m=\u001b[39m \u001b[43menc_gen_train_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m epoch_e_loss\u001b[38;5;241m.\u001b[39mappend(enc_loss)\n\u001b[0;32m     43\u001b[0m epoch_g_loss\u001b[38;5;241m.\u001b[39mappend(gen_loss)\n",
      "File \u001b[1;32mc:\\Users\\farra\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\farra\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:892\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_functions_eagerly:\n\u001b[0;32m    891\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, tf_function_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    894\u001b[0m \u001b[38;5;66;03m# Only count the statistics the first time, before initialization took\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;66;03m# place.\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[28], line 18\u001b[0m, in \u001b[0;36menc_gen_train_on_batch\u001b[1;34m(x, z)\u001b[0m\n\u001b[0;32m     14\u001b[0m     loss3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10.0\u001b[39m\u001b[38;5;241m*\u001b[39mtf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMSE(x, x_gen_rec))\n\u001b[0;32m     16\u001b[0m     enc_loss \u001b[38;5;241m=\u001b[39m loss1 \u001b[38;5;241m+\u001b[39m loss2 \u001b[38;5;241m+\u001b[39m loss3\n\u001b[1;32m---> 18\u001b[0m gradients_encoder \u001b[38;5;241m=\u001b[39m \u001b[43menc_tape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients_encoder, encoder\u001b[38;5;241m.\u001b[39mtrainable_weights))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m gen_tape:\n",
      "File \u001b[1;32mc:\\Users\\farra\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1113\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1107\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1108\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1109\u001b[0m           output_gradients))\n\u001b[0;32m   1110\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1111\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1113\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32mc:\\Users\\farra\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\farra\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:130\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    121\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_control_flow_context\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.GradientTape.gradients() does not support graph control flow \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperations like tf.cond or tf.while at this time. Use tf.gradients() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead. If you need this feature, please file a feature request at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/tensorflow/tensorflow/issues/new\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m     )\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gradient_function\u001b[39m(op_name, attr_tuple, num_inputs, inputs, outputs,\n\u001b[0;32m    131\u001b[0m                        out_grads, skip_input_indices, forward_pass_name_scope):\n\u001b[0;32m    132\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the gradient function of the op.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    The gradients with respect to the inputs of the function, as a list.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m    148\u001b[0m   mock_op \u001b[38;5;241m=\u001b[39m _MockOp(attr_tuple, inputs, outputs, op_name, skip_input_indices)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fake = np.ones((batch_size, 1), dtype=np.float32)\n",
    "# valid = -np.ones((batch_size, 1), dtype=np.float32)\n",
    "# delta = np.ones((batch_size, 1), dtype=np.float32)\n",
    "\n",
    "epoch_e_loss = []    \n",
    "epoch_g_loss = []\n",
    "epoch_cx_loss = []\n",
    "epoch_cz_loss = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    np.random.shuffle(X_)\n",
    "    \n",
    "    minibatches_size = batch_size * n_critics  # 64*5 = 320 \n",
    "    num_minibatches = int(X_.shape[0] // minibatches_size)  # 12 \n",
    "    \n",
    "    encoder.trainable = False\n",
    "    generator.trainable = False\n",
    "    \n",
    "    for i in range(num_minibatches):\n",
    "        minibatch = X_[i * minibatches_size: (i + 1) * minibatches_size]\n",
    "                \n",
    "        # Number of Critics \n",
    "        for j in range(n_critics):\n",
    "            \n",
    "            x = minibatch[j * batch_size: (j + 1) * batch_size]\n",
    "            z = tf.random.normal(shape=(batch_size, latent_dim, 1), mean=0.0, stddev=1, dtype=tf.dtypes.float32, seed=1748)\n",
    "            \n",
    "            critic_x.trainable = True\n",
    "            critic_z.trainable = False\n",
    "            epoch_cx_loss.append(critic_x_train_on_batch(x, z))\n",
    "            critic_x.trainable = False\n",
    "            critic_z.trainable = True\n",
    "            epoch_cz_loss.append(critic_z_train_on_batch(x, z))\n",
    "        \n",
    "        critic_z.trainable = False\n",
    "        critic_x.trainable = False\n",
    "        encoder.trainable = True\n",
    "        generator.trainable = True\n",
    "        \n",
    "        enc_loss, gen_loss = enc_gen_train_on_batch(x, z)\n",
    "        epoch_e_loss.append(enc_loss)\n",
    "        epoch_g_loss.append(gen_loss)\n",
    "        \n",
    "    cx_loss = np.mean(np.array(epoch_cx_loss), axis=0)\n",
    "    cz_loss = np.mean(np.array(epoch_cz_loss), axis=0)\n",
    "    e_loss = np.mean(np.array(epoch_e_loss), axis=0)\n",
    "    g_loss = np.mean(np.array(epoch_g_loss), axis=0)\n",
    "    \n",
    "    print('Epoch: {}/{}, [Dx loss: {}] [Dz loss: {}] [E loss: {}] [G loss: {}]'.format(epoch, epochs, cx_loss, cz_loss, e_loss, g_loss))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4LOxxEKOvWb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TadGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
