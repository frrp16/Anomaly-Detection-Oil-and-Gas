{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, BatchNormalization, LeakyReLU, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "time_steps = 100   # Number of timesteps in input data\n",
    "features = 10      # Number of features in each timestep\n",
    "latent_dim = 64    # Dimension of latent space\n",
    "\n",
    "# Generator (Autoencoder with Encoder1 and Decoder)\n",
    "def build_generator():\n",
    "    inputs = Input(shape=(time_steps, features))\n",
    "    \n",
    "    # Encoder 1 (LSTM layers + Fully Connected Layers)\n",
    "    x = LSTM(128, return_sequences=True)(inputs)\n",
    "    x = LSTM(64)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    # Latent representation (z)\n",
    "    latent = Dense(latent_dim, activation='relu')(x)\n",
    "\n",
    "    # Decoder (Symmetrical to Encoder 1)\n",
    "    x = Dense(64)(latent)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dense(time_steps * features)(x)\n",
    "    outputs = Reshape((time_steps, features))(x)\n",
    "    \n",
    "    model = Model(inputs, outputs, name=\"Generator\")\n",
    "    return model\n",
    "\n",
    "# Discriminator (used to classify real vs fake)\n",
    "def build_discriminator():\n",
    "    inputs = Input(shape=(time_steps, features))\n",
    "    \n",
    "    x = LSTM(128, return_sequences=True)(inputs)\n",
    "    x = LSTM(64)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    # Flatten and classify real vs fake\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, x, name=\"Discriminator\")\n",
    "    return model\n",
    "\n",
    "# Define loss functions\n",
    "def generator_loss(fake_output, reconstructed_input, real_latent, reconstructed_latent, lambdas):\n",
    "    bce_loss = BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "    # Reconstruction loss (input space, L1 loss)\n",
    "    lx = tf.reduce_mean(tf.abs(reconstructed_input - real_latent))\n",
    "    \n",
    "    # Latent space reconstruction loss (L2 loss)\n",
    "    lz = tf.reduce_mean(tf.square(reconstructed_latent - real_latent))\n",
    "    \n",
    "    # Adversarial loss (Binary Crossentropy)\n",
    "    adversarial_loss = bce_loss(tf.ones_like(fake_output), fake_output)\n",
    "    \n",
    "    # Combine losses\n",
    "    loss = lambdas[0] * lx + lambdas[1] * lz + lambdas[2] * adversarial_loss\n",
    "    return loss\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    bce_loss = BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "    # Binary crossentropy loss for real and fake samples\n",
    "    real_loss = bce_loss(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = bce_loss(tf.zeros_like(fake_output), fake_output)\n",
    "    \n",
    "    # Total discriminator loss\n",
    "    return real_loss + fake_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 100, 10)]         0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 100, 128)          71168     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1000)              65000     \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 100, 10)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,568\n",
      "Trainable params: 198,312\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Model: \"Discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 100, 10)]         0         \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 100, 128)          71168     \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 125,057\n",
      "Trainable params: 124,929\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "generator.summary()\n",
    "discriminator.summary()\n",
    "# Optimizers\n",
    "gen_optimizer = Adam(0.0002, beta_1=0.5)\n",
    "disc_optimizer = Adam(0.0002, beta_1=0.5)\n",
    "\n",
    "# Placeholders for real and fake inputs\n",
    "real_data = Input(shape=(time_steps, features))\n",
    "fake_data = generator(real_data)\n",
    "\n",
    "# Discriminator training\n",
    "real_output = discriminator(real_data)\n",
    "fake_output = discriminator(fake_data)\n",
    "\n",
    "# Custom training loop\n",
    "@tf.function\n",
    "def train_step(real_data):\n",
    "    # Latent vectors for reconstruction\n",
    "    real_latent = generator(real_data)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generate fake data\n",
    "        fake_data = generator(real_data)\n",
    "        \n",
    "        # Discriminator outputs\n",
    "        real_output = discriminator(real_data)\n",
    "        fake_output = discriminator(fake_data)\n",
    "        \n",
    "        # Latent space reconstruction\n",
    "        reconstructed_latent = generator(fake_data)\n",
    "        \n",
    "        # Calculate losses\n",
    "        gen_loss = generator_loss(fake_output, fake_data, real_latent, reconstructed_latent, lambdas=[0.1, 0.1, 0.8])\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    # Update gradients\n",
    "    gradients_of_generator = tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\farra\\anaconda3\\lib\\site-packages\\keras\\backend.py:5673: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gen Loss: 0.650508463382721, Disc Loss: 1.1934301853179932\n",
      "Epoch 2, Gen Loss: 2.7452542781829834, Disc Loss: 0.5759519338607788\n",
      "Epoch 3, Gen Loss: 0.0530317984521389, Disc Loss: 3.0407092571258545\n",
      "Epoch 4, Gen Loss: 0.6268647909164429, Disc Loss: 0.9519755244255066\n",
      "Epoch 5, Gen Loss: 0.7185719609260559, Disc Loss: 0.8008942604064941\n",
      "Epoch 6, Gen Loss: 1.5878078937530518, Disc Loss: 0.3267285227775574\n",
      "Epoch 7, Gen Loss: 1.6642814874649048, Disc Loss: 0.2551543712615967\n",
      "Epoch 8, Gen Loss: 2.2694976329803467, Disc Loss: 0.15700165927410126\n",
      "Epoch 9, Gen Loss: 2.5059597492218018, Disc Loss: 0.12790241837501526\n",
      "Epoch 10, Gen Loss: 2.66983699798584, Disc Loss: 0.11056220531463623\n",
      "Epoch 11, Gen Loss: 2.7254414558410645, Disc Loss: 0.1016135960817337\n",
      "Epoch 12, Gen Loss: 2.8578872680664062, Disc Loss: 0.09121386706829071\n",
      "Epoch 13, Gen Loss: 3.1955230236053467, Disc Loss: 0.08169358223676682\n",
      "Epoch 14, Gen Loss: 3.1780996322631836, Disc Loss: 0.02545725554227829\n",
      "Epoch 15, Gen Loss: 3.2758240699768066, Disc Loss: 0.0341026708483696\n",
      "Epoch 16, Gen Loss: 3.4684271812438965, Disc Loss: 0.05308683216571808\n",
      "Epoch 17, Gen Loss: 3.8970746994018555, Disc Loss: 0.020605631172657013\n",
      "Epoch 18, Gen Loss: 4.158564567565918, Disc Loss: 0.016988325864076614\n",
      "Epoch 19, Gen Loss: 4.170297145843506, Disc Loss: 0.014589732512831688\n",
      "Epoch 20, Gen Loss: 4.175635814666748, Disc Loss: 0.012055570259690285\n",
      "Epoch 21, Gen Loss: 4.64274263381958, Disc Loss: 0.007635482121258974\n",
      "Epoch 22, Gen Loss: 5.109816551208496, Disc Loss: 0.00453163729980588\n",
      "Epoch 23, Gen Loss: 5.324582576751709, Disc Loss: 0.0036652549169957638\n",
      "Epoch 24, Gen Loss: 5.333632469177246, Disc Loss: 0.0033294239547103643\n",
      "Epoch 25, Gen Loss: 5.412014961242676, Disc Loss: 0.002999001182615757\n",
      "Epoch 26, Gen Loss: 5.5716986656188965, Disc Loss: 0.002612270414829254\n",
      "Epoch 27, Gen Loss: 5.584974765777588, Disc Loss: 0.0024508503265678883\n",
      "Epoch 28, Gen Loss: 5.480560302734375, Disc Loss: 0.0024613456334918737\n",
      "Epoch 29, Gen Loss: 5.690942764282227, Disc Loss: 0.002109996974468231\n",
      "Epoch 30, Gen Loss: 5.741917133331299, Disc Loss: 0.001963655697181821\n",
      "Epoch 31, Gen Loss: 5.85722017288208, Disc Loss: 0.0017770554404705763\n",
      "Epoch 32, Gen Loss: 5.903740406036377, Disc Loss: 0.001667478820309043\n",
      "Epoch 33, Gen Loss: 6.0126166343688965, Disc Loss: 0.001524067483842373\n",
      "Epoch 34, Gen Loss: 6.0916290283203125, Disc Loss: 0.0014144456945359707\n",
      "Epoch 35, Gen Loss: 6.071620464324951, Disc Loss: 0.00137424748390913\n",
      "Epoch 36, Gen Loss: 6.152925491333008, Disc Loss: 0.0012776361545547843\n",
      "Epoch 37, Gen Loss: 6.266379356384277, Disc Loss: 0.0011731229024007916\n",
      "Epoch 38, Gen Loss: 6.294363498687744, Disc Loss: 0.0011182462330907583\n",
      "Epoch 39, Gen Loss: 6.339456558227539, Disc Loss: 0.0010594225022941828\n",
      "Epoch 40, Gen Loss: 6.425736427307129, Disc Loss: 0.000987364794127643\n",
      "Epoch 41, Gen Loss: 6.486060619354248, Disc Loss: 0.0009313385235145688\n",
      "Epoch 42, Gen Loss: 6.539003849029541, Disc Loss: 0.0008817765046842396\n",
      "Epoch 43, Gen Loss: 6.710859298706055, Disc Loss: 0.0007987226126715541\n",
      "Epoch 44, Gen Loss: 6.749077796936035, Disc Loss: 0.000761352595873177\n",
      "Epoch 45, Gen Loss: 6.823194980621338, Disc Loss: 0.0007172358455136418\n",
      "Epoch 46, Gen Loss: 6.812198162078857, Disc Loss: 0.0006968547822907567\n",
      "Epoch 47, Gen Loss: 6.9638214111328125, Disc Loss: 0.0006403414299711585\n",
      "Epoch 48, Gen Loss: 7.076874732971191, Disc Loss: 0.0005977303953841329\n",
      "Epoch 49, Gen Loss: 7.334605693817139, Disc Loss: 0.0005488843307830393\n",
      "Epoch 50, Gen Loss: 7.321519374847412, Disc Loss: 0.0005275243311189115\n",
      "32/32 [==============================] - 2s 36ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and multiclass-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Evaluate the model (replace `true_labels` with actual labels)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 38\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(real_data, true_labels, thresholds)\u001b[0m\n\u001b[0;32m     35\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m label_data(anomaly_scores, thresholds)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Calculate evaluation metrics\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweighted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(true_labels, predicted_labels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(true_labels, predicted_labels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\farra\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1776\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprecision_score\u001b[39m(\n\u001b[0;32m   1648\u001b[0m     y_true,\n\u001b[0;32m   1649\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1655\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1656\u001b[0m ):\n\u001b[0;32m   1657\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m \n\u001b[0;32m   1659\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1776\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1777\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1778\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\Users\\farra\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1563\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1562\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1563\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1566\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\farra\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1364\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1364\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1367\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\farra\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and multiclass-multioutput targets"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for real_data in dataset:\n",
    "            gen_loss, disc_loss = train_step(real_data)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Gen Loss: {gen_loss}, Disc Loss: {disc_loss}')\n",
    "\n",
    "# Anomaly score and labeling\n",
    "def compute_anomaly_score(real_data):\n",
    "    fake_data = generator.predict(real_data)\n",
    "    \n",
    "    # Latent space reconstruction loss\n",
    "    latent_real = generator(real_data)\n",
    "    latent_fake = generator(fake_data)\n",
    "    \n",
    "    anomaly_score = np.mean(np.square(latent_real - latent_fake), axis=1)\n",
    "    \n",
    "    return anomaly_score\n",
    "\n",
    "def label_data(anomaly_score, thresholds):\n",
    "    labels = np.zeros_like(anomaly_score)\n",
    "    labels[anomaly_score < thresholds[0]] = 0  # \"Good\"\n",
    "    labels[(anomaly_score >= thresholds[0]) & (anomaly_score < thresholds[1])] = 1  # \"Watching\"\n",
    "    labels[(anomaly_score >= thresholds[1]) & (anomaly_score < thresholds[2])] = 2  # \"Warning\"\n",
    "    labels[anomaly_score >= thresholds[2]] = 3  # \"Fault\"\n",
    "    return labels\n",
    "\n",
    "# Model Evaluation\n",
    "def evaluate_model(real_data, true_labels, thresholds):\n",
    "    # Compute anomaly scores\n",
    "    anomaly_scores = compute_anomaly_score(real_data)\n",
    "    \n",
    "    # Label data based on thresholds\n",
    "    predicted_labels = label_data(anomaly_scores, thresholds)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "# Sample dataset and training example\n",
    "train_data = np.random.randn(1000, time_steps, features)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(32)\n",
    "\n",
    "# Train the model\n",
    "train(train_dataset, epochs=50)\n",
    "\n",
    "# Evaluate the model (replace `true_labels` with actual labels)\n",
    "true_labels = np.random.randint(0, 4, size=1000)\n",
    "# evaluate_model(train_data, true_labels, thresholds=[0.2, 0.5, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_window(data, timesteps):\n",
    "    samples = data.shape[0] - timesteps + 1  # Number of samples in the new 3D array\n",
    "    variables = data.shape[1]  # Number of variables (features)\n",
    "\n",
    "    data_3d = np.zeros((samples, timesteps, variables))\n",
    "\n",
    "    for i in range(samples):\n",
    "        data_3d[i] = data[i:i+timesteps]\n",
    "    return data_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_convert_time_window(data_3d):\n",
    "    samples, timesteps, variables = data_3d.shape\n",
    "    data_2d = np.zeros((samples + timesteps - 1, variables))\n",
    "\n",
    "    count = np.zeros((samples + timesteps - 1, variables))\n",
    "\n",
    "    for i in range(samples):\n",
    "        data_2d[i:i+timesteps] += data_3d[i]\n",
    "        count[i:i+timesteps] += 1\n",
    "\n",
    "    # Average the overlapping segments\n",
    "    data_2d /= count\n",
    "    return data_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "# train_data_2d = inverse_convert_time_window(train_data)\n",
    "\n",
    "fake_data = generator.predict(train_data)\n",
    "\n",
    "fake_data_2d = np.mean(fake_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "fake_data = generator.predict(train_data)\n",
    "\n",
    "# Latent space reconstruction loss\n",
    "latent_real = generator(train_data)\n",
    "latent_fake = generator(fake_data)\n",
    "\n",
    "latent_real_2d = inverse_convert_time_window(latent_real)\n",
    "latent_fake_2d = inverse_convert_time_window(latent_fake)\n",
    "\n",
    "anomaly_score = np.mean(np.square(latent_real_2d - latent_fake_2d), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0531, Recall: 0.2300, F1-Score: 0.0862\n",
      "Confusion Matrix:\n",
      "[[230   0   0   0]\n",
      " [240   0   0   0]\n",
      " [262   3   0   0]\n",
      " [265   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\farra\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = label_data(anomaly_score, thresholds=[0.2, 0.5, 0.7])\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(true_labels, predicted_labels[:1000], average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels[:1000], average='weighted')\n",
    "f1 = f1_score(true_labels, predicted_labels[:1000], average='weighted')\n",
    "cm = confusion_matrix(true_labels, predicted_labels[:1000])\n",
    "\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "# fake_data = generator.predict(train_data)\n",
    "\n",
    "# anomaly_scores = compute_anomaly_score(train_data)\n",
    "# predicted_labels = label_data(anomaly_scores, thresholds=[0.2, 0.5, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set device\n",
    "# device = \"/gpu:0\" if tf.config.list_physical_devices('GPU') else \"/cpu:0\"\n",
    "\n",
    "# # LSTM Encoder\n",
    "# class LSTMEncoder(tf.keras.Model):\n",
    "#     def __init__(self, input_size, hidden_size, latent_size, num_layers):\n",
    "#         super(LSTMEncoder, self).__init__()\n",
    "#         self.lstm = layers.LSTM(hidden_size, num_layers, return_sequences=False, return_state=True)\n",
    "#         self.fc = models.Sequential([\n",
    "#             layers.Dense(latent_size),\n",
    "#             layers.BatchNormalization(),\n",
    "#             layers.ReLU()\n",
    "#         ])\n",
    "        \n",
    "#     def call(self, x):\n",
    "#         _, h, _ = self.lstm(x)\n",
    "#         z = self.fc(h)\n",
    "#         return z\n",
    "\n",
    "# # Decoder (Generator)\n",
    "# class LSTMDecoder(tf.keras.Model):\n",
    "#     def __init__(self, latent_size, hidden_size, output_size, num_layers):\n",
    "#         super(LSTMDecoder, self).__init__()\n",
    "#         self.fc = layers.Dense(hidden_size)\n",
    "#         self.lstm = layers.LSTM(output_size, num_layers, activation='relu', return_sequences=True)\n",
    "        \n",
    "#     def call(self, z, seq_len):\n",
    "#         h = tf.nn.relu(self.fc(z))\n",
    "#         h = tf.expand_dims(h, 1)\n",
    "#         h = tf.tile(h, [1, seq_len, 1])  # Expand to sequence length\n",
    "#         out = self.lstm(h)\n",
    "#         return out\n",
    "\n",
    "# # Discriminator\n",
    "# class Discriminator(tf.keras.Model):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.lstm = layers.LSTM(hidden_size, num_layers, return_state=False, activation='relu', return_sequences=False)\n",
    "#         self.fc = layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "#     def call(self, x):\n",
    "#         h = self.lstm(x)\n",
    "#         out = self.fc(h)\n",
    "#         return out\n",
    "\n",
    "# # LSTM-GAN Model\n",
    "# class LSTM_GAN(tf.keras.Model):\n",
    "#     def __init__(self, input_size, hidden_size, latent_size, num_layers):\n",
    "#         super(LSTM_GAN, self).__init__()\n",
    "#         self.encoder = LSTMEncoder(input_size, hidden_size, latent_size, num_layers)\n",
    "#         self.decoder = LSTMDecoder(latent_size, hidden_size, input_size, num_layers)\n",
    "#         self.discriminator = Discriminator(input_size, hidden_size, num_layers)\n",
    "    \n",
    "#     def call(self, x):\n",
    "#         z = self.encoder(x)\n",
    "#         reconstructed_x = self.decoder(z, tf.shape(x)[1])\n",
    "#         return reconstructed_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss Functions\n",
    "# def generator_loss(reconstructed_x, real_x, z, z_hat, discriminator_out, w1=1.0, w2=1.0, w3=1.0):\n",
    "#     l1_loss = tf.reduce_mean(tf.abs(reconstructed_x - real_x))\n",
    "#     l2_loss = tf.reduce_mean(tf.square(z - z_hat))\n",
    "#     adv_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(discriminator_out), discriminator_out)\n",
    "#     return w1 * l1_loss + w2 * l2_loss + w3 * adv_loss\n",
    "\n",
    "# def discriminator_loss(real_output, fake_output):\n",
    "#     real_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(real_output), real_output)\n",
    "#     fake_loss = tf.keras.losses.BinaryCrossentropy()(tf.zeros_like(fake_output), fake_output)\n",
    "#     return real_loss + fake_loss\n",
    "\n",
    "# # Data Preparation (synthetic for demo purposes)\n",
    "# def generate_synthetic_data(num_samples, seq_len, input_size):\n",
    "#     data = np.sin(np.linspace(0, 100, num_samples * seq_len).reshape(num_samples, seq_len, input_size))\n",
    "#     return data + 0.05 * np.random.randn(num_samples, seq_len, input_size)\n",
    "\n",
    "# # Labeling Reconstructed Data\n",
    "# def label_reconstructed_data(real_x, reconstructed_x, thresholds):\n",
    "#     errors = tf.reduce_mean(tf.abs(real_x - reconstructed_x), axis=[1, 2]).numpy()\n",
    "#     labels = np.zeros(len(errors))\n",
    "#     labels[errors >= thresholds[2]] = 3  # Fault\n",
    "#     labels[(errors >= thresholds[1]) & (errors < thresholds[2])] = 2  # Warning\n",
    "#     labels[(errors >= thresholds[0]) & (errors < thresholds[1])] = 1  # Watching\n",
    "#     labels[errors < thresholds[0]] = 0  # Good\n",
    "#     return labels\n",
    "\n",
    "# # Model Evaluation\n",
    "# def evaluate_model(real_labels, predicted_labels):\n",
    "#     print(\"Confusion Matrix:\\n\", confusion_matrix(real_labels, predicted_labels))\n",
    "#     print(\"Classification Report:\\n\", classification_report(real_labels, predicted_labels))\n",
    "\n",
    "# # Main training loop\n",
    "# def train_lstm_gan(model, dataset, epochs, input_size, hidden_size, latent_size, num_layers):\n",
    "#     optimizer_G = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#     optimizer_D = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         for data, labels in dataset:\n",
    "#             real_x = data\n",
    "            \n",
    "#             with tf.GradientTape() as tape_D, tf.GradientTape() as tape_G:\n",
    "#                 # Train Discriminator\n",
    "#                 z = model.encoder(real_x)\n",
    "#                 reconstructed_x = model.decoder(z, tf.shape(real_x)[1])\n",
    "#                 fake_output = model.discriminator(reconstructed_x)\n",
    "#                 real_output = model.discriminator(real_x)\n",
    "#                 d_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "#                 # Train Generator\n",
    "#                 z_hat = model.encoder(reconstructed_x)\n",
    "#                 g_loss = generator_loss(reconstructed_x, real_x, z, z_hat, fake_output)\n",
    "            \n",
    "#             gradients_D = tape_D.gradient(d_loss, model.discriminator.trainable_variables)\n",
    "#             optimizer_D.apply_gradients(zip(gradients_D, model.discriminator.trainable_variables))\n",
    "            \n",
    "#             gradients_G = tape_G.gradient(g_loss, model.encoder.trainable_variables + model.decoder.trainable_variables)\n",
    "#             optimizer_G.apply_gradients(zip(gradients_G, model.encoder.trainable_variables + model.decoder.trainable_variables))\n",
    "        \n",
    "#         if epoch % 10 == 0:\n",
    "#             print(f'Epoch [{epoch}/{epochs}], Discriminator Loss: {d_loss:.4f}, Generator Loss: {g_loss:.4f}')\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters and Data\n",
    "# input_size = 1\n",
    "# hidden_size = 64\n",
    "# latent_size = 32\n",
    "# num_layers = 2\n",
    "# num_samples = 1000\n",
    "# seq_len = 50\n",
    "# epochs = 100\n",
    "\n",
    "# data = generate_synthetic_data(num_samples, seq_len, input_size)\n",
    "# labels = np.zeros(num_samples)  # Synthetic labels\n",
    "\n",
    "# # Prepare Dataset and Dataloader\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((data, labels)).batch(32).shuffle(100)\n",
    "\n",
    "# # Initialize and Train the Model\n",
    "# with tf.device(device):\n",
    "#     model = LSTM_GAN(input_size, hidden_size, latent_size, num_layers)\n",
    "#     trained_model = train_lstm_gan(model, dataset, epochs, input_size, hidden_size, latent_size, num_layers)\n",
    "\n",
    "# # Evaluate the Model\n",
    "# with tf.device(device):\n",
    "#     real_data = tf.convert_to_tensor(data, dtype=tf.float32)\n",
    "#     reconstructed_data = trained_model(real_data)\n",
    "#     thresholds = [0.02, 0.05, 0.1]  # Example thresholds for labeling\n",
    "#     predicted_labels = label_reconstructed_data(real_data, reconstructed_data, thresholds)\n",
    "#     evaluate_model(labels, predicted_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
